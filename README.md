## XGBoost Pipeline â€“ Step by Step Explanation

Bu hujjat **XGBoost + Pipeline Breast Cancer Classification** loyihasining **qatorma-qator tushuntirishi** hisoblanadi.

---

# 1ï¸âƒ£ Kutubxonalar (Imports)

```python
import numpy as np
```

**NumPy** â€” sonli hisob-kitoblar uchun ishlatiladi.

Bu loyihada asosan:

```
np.argsort()
```

feature importance qiymatlarini saralash uchun ishlatiladi.

---

```python
from sklearn.datasets import load_breast_cancer
```

`load_breast_cancer()` â€” sklearn ichidagi tayyor **Breast Cancer datasetni yuklaydi.**

---

```python
from sklearn.model_selection import train_test_split
```

Datasetni:

* Train set
* Test set

ga boâ€˜lib beradi.

---

```python
from sklearn.pipeline import Pipeline
```

Pipeline bir nechta bosqichni bitta tizimga bogâ€˜laydi:

```
Scaler â†’ Model
```

Pipeline **data leakage ni oldini oladi.**

---

```python
from sklearn.preprocessing import StandardScaler
```

Featurelarni standartlashtiradi.

Formula:

```
z = (x - mean) / std
```

Natija:

* Oâ€˜rtacha â‰ˆ 0
* Standart ogâ€˜ish â‰ˆ 1

---

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score
```

Modelni baholash uchun ishlatiladi.

### Accuracy

Umumiy toâ€˜gâ€˜ri bashorat foizi.

### Precision

"1 deb aytilganlarning nechasi toâ€˜gâ€˜ri"

### Recall

"Aslida 1 boâ€˜lganlarning nechasi topildi"

---

```python
from xgboost import XGBClassifier
```

XGBoost klassifikatsiya modeli.

---

# 2ï¸âƒ£ Datasetni Yuklash

```python
data = load_breast_cancer()
```

Datasetni yuklaydi.

Dataset ichida:

```
data.data
```

Featurelar.

```
data.target
```

Target qiymatlar.

```
data.feature_names
```

Feature nomlari.

---

```python
X = data.data
y = data.target
feature_names = data.feature_names
```

### X

Model kirish qiymatlari.

30 ta feature mavjud.

---

### y

Target label.

```
0 = Malignant
1 = Benign
```

---

### feature_names

Feature nomlari.

Keyinchalik **Top Features chiqarish uchun kerak.**

---

# 3ï¸âƒ£ Train/Test Split

```python
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
```

Dataset boâ€˜linadi:

```
80% Train
20% Test
```

---

## test_size=0.2

20% test data.

---

## random_state=42

Har safar run qilinganda bir xil natija chiqadi.

Bu **reproducibility** deyiladi.

---

## stratify=y

Classlar nisbatini saqlaydi.

Masalan:

Agar datasetda:

```
60% class 1
40% class 0
```

Train va testda ham shunga yaqin boâ€˜ladi.

Classification uchun muhim.

---

## Natija

```
X_train , y_train â†’ Training
X_test , y_test â†’ Testing
```

---

# 4ï¸âƒ£ Pipeline Yaratish

```python
pipe = Pipeline(steps=[

    ("scaler", StandardScaler()),

    ("xgb", XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.9,
        colsample_bytree=0.9,
        random_state=42,
        eval_metric="logloss",
        n_jobs=-1
    ))

])
```

Pipeline ketma-ketligi:

```
StandardScaler
â†“
XGBClassifier
```

---

## Pipeline qanday ishlaydi?

### pipe.fit()

Pipeline quyidagilarni qiladi:

1.

```
scaler.fit(X_train)
```

Train datadan mean va std topadi.

---

2.

```
scaler.transform(X_train)
```

Train datani standartlashtiradi.

---

3.

```
xgb.fit(...)
```

Modelni oâ€˜qitadi.

---

### pipe.predict()

Pipeline quyidagilarni qiladi:

1.

```
scaler.transform(X_test)
```

Test datani standartlashtiradi.

---

2.

```
xgb.predict()
```

Predict qiladi.

---

Pipeline **data leakage ni oldini oladi.**

---

# 5ï¸âƒ£ XGBClassifier Parametrlari

### n_estimators = 300

300 ta daraxt quriladi.

Boosting bosqichlari soni.

---

### learning_rate = 0.05

Har daraxtning hissasi kichik boâ€˜ladi.

Sekinroq oâ€˜rganadi lekin barqarorroq.

---

### max_depth = 4

Daraxt chuqurligi.

Juda katta boâ€˜lsa overfitting boâ€˜ladi.

---

### subsample = 0.9

Har daraxt uchun dataning 90% ishlatiladi.

Overfitting kamayadi.

---

### colsample_bytree = 0.9

Har daraxt uchun featurelarning 90% ishlatiladi.

---

### random_state = 42

Natijani bir xil qiladi.

---

### eval_metric = "logloss"

Baholash metrikasi.

Warning chiqmasligi uchun ham kerak.

---

### n_jobs = -1

Barcha CPU ishlatiladi.

Tezroq ishlaydi.

---

# 6ï¸âƒ£ Model Training

```python
pipe.fit(X_train, y_train)
```

Pipeline quyidagilarni bajaradi:

```
scaler.fit(X_train)
```

â†“

```
scaler.transform(X_train)
```

â†“

```
xgb.fit(...)
```

---

# 7ï¸âƒ£ Prediction

```python
y_pred = pipe.predict(X_test)
```

Pipeline quyidagilarni qiladi:

```
scaler.transform(X_test)
```

â†“

```
xgb.predict()
```

Natija:

```
y_pred
```

0 yoki 1 bashoratlar.

---

# 8ï¸âƒ£ Metrics

```python
acc = accuracy_score(y_test, y_pred)
```

Accuracy:

Umumiy toâ€˜gâ€˜ri bashorat foizi.

---

```python
prec = precision_score(y_test, y_pred)
```

Precision:

1 deb aytilganlarning nechasi toâ€˜gâ€˜ri.

---

```python
rec = recall_score(y_test, y_pred)
```

Recall:

Aslida 1 boâ€˜lganlarning nechasi topildi.

---

```python
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
```

Natijani 4 xonagacha chiqaradi.

---

# 9ï¸âƒ£ Feature Importance

```python
model = pipe.named_steps["xgb"]
```

Pipeline ichidan model olinadi.

Pipeline bosqichlari lugâ€˜ati:

```
named_steps
```

---

```python
importances = model.feature_importances_
```

Har bir feature importance qiymatini beradi.

Uzunligi:

```
30 ta feature
```

---

# ğŸ”Ÿ Top 5 Feature

```python
top5_idx = np.argsort(importances)[-5:][::-1]
```

### np.argsort(importances)

Importance qiymatlarini kichikdan kattaga saralaydi.

Indexlarni qaytaradi.

---

### [-5:]

Eng katta 5 ta qiymat olinadi.

---

### [::-1]

Teskari aylantiriladi.

Kattadan kichikka.

---

```python
top5_features = feature_names[top5_idx]
```

Top feature nomlari olinadi.

---

```python
print("\nTop 5 muhim featurelar:")

for i, idx in enumerate(top5_idx, 1):

    print(f"{i}) {feature_names[idx]} (importance={importances[idx]:.4f})")
```

### enumerate(...,1)

Sanash 1 dan boshlanadi.
Natija

1) worst perimeter
2) worst concave points
3) mean concave points
4) worst radius
5) mean perimeter
# ğŸ¯ Xulosa

Bu loyiha quyidagilarni koâ€˜rsatadi:
* Pipeline ishlatish
* StandardScaler
* XGBoost
* Train/Test Split
* Accuracy
* Precision
* Recall
* Feature Importance


## XGBoost Pipeline â€“ Breast Cancer Classification

---

## ğŸ“Œ Project Overview

This project demonstrates a **machine learning classification pipeline** using **XGBoost** on the **Breast Cancer dataset** from Scikit-learn.

The goal is to:

* Build a **Pipeline**
* Perform **Train/Test Split**
* Train an **XGBoost model**
* Evaluate the model
* Extract **Top Important Features**

---

## ğŸ“Š Dataset

Dataset used:

```
sklearn.datasets.load_breast_cancer()
```

Dataset contains:

* 569 samples
* 30 numerical features
* Binary classification target:

  * 0 = Malignant
  * 1 = Benign

---

## âš™ï¸ Technologies Used

* Python
* Scikit-learn
* XGBoost
* NumPy

---

## ğŸ“¦ Import Libraries

```python
import numpy as np

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score

from xgboost import XGBClassifier
```

---

## ğŸ“¥ Load Dataset

```python
data = load_breast_cancer()

X = data.data
y = data.target

feature_names = data.feature_names
```

---

## âœ‚ï¸ Train Test Split

Dataset is split into:

* 80% Training Data
* 20% Test Data

```python
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
```

---

## ğŸ”„ Pipeline Creation

Pipeline consists of:

1. StandardScaler
2. XGBClassifier

```python
pipe = Pipeline(steps=[

    ("scaler", StandardScaler()),

    ("xgb", XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.9,
        colsample_bytree=0.9,
        random_state=42,
        eval_metric="logloss",
        n_jobs=-1
    ))

])
```

---

## ğŸ§  Model Training

```python
pipe.fit(X_train, y_train)
```

Pipeline automatically:

* Fits scaler on training data
* Transforms training data
* Trains XGBoost model

---

## ğŸ”® Prediction

```python
y_pred = pipe.predict(X_test)
```

---

## ğŸ“ˆ Model Evaluation

### Accuracy

```python
accuracy_score(y_test, y_pred)
```

### Precision

```python
precision_score(y_test, y_pred)
```

### Recall

```python
recall_score(y_test, y_pred)
```

Example output:

```
Accuracy : 0.9737
Precision: 0.9726
Recall   : 0.9861
```

---

## â­ Feature Importance

Extract XGBoost model from Pipeline:

```python
model = pipe.named_steps["xgb"]
```

Get importance values:

```python
importances = model.feature_importances_
```

Top 5 features:

```python
top5_idx = np.argsort(importances)[-5:][::-1]

for i, idx in enumerate(top5_idx, 1):
    print(feature_names[idx], importances[idx])
```

---

## ğŸ¥‡ Top Important Features (Example)

```
worst perimeter
worst concave points
mean concave points
worst radius
mean perimeter
```

---

## ğŸš€ How to Run

Install libraries:

```
pip install xgboost scikit-learn numpy
```

Run:

```
python main.py
```

---

## ğŸ“Œ Key Concepts Demonstrated

* Pipeline
* StandardScaler
* XGBoost
* Train/Test Split
* Accuracy
* Precision
* Recall
* Feature Importance

---

## ğŸ‘¨â€ğŸ’» Author

Machine Learning Student
